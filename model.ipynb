{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used while training\n",
    "\n",
    "1. loss\n",
    "2. optimizer and training\n",
    "3. evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. loss function\n",
    "\n",
    "Compute loss from logits and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def losses(logits, labels):\n",
    "    '''\n",
    "    Args:\n",
    "        logits: logits tensor, float, [batch_size, n_classes]\n",
    "        labels: label tensor, tf.int32, [batch_size]\n",
    "        \n",
    "    Returns:\n",
    "        loss tensor of float type\n",
    "    '''\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits\\\n",
    "                        (logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        #tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. optimizer and training\n",
    "\n",
    "Training ops, the Op returned by this function is what must be passed to 'sess.run()' call to cause the model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainning(loss, learning_rate):\n",
    "    '''\n",
    "    Args:\n",
    "        loss: loss tensor, from losses()\n",
    "        \n",
    "    Returns:\n",
    "        train_op: The op for trainning\n",
    "    '''\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. evaluation\n",
    "\n",
    "Evaluate the quality of the logits at predicting the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    '''\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "        range [0, NUM_CLASSES).\n",
    "    Returns:\n",
    "        A scalar int32 tensor with the number of examples (out of batch_size) \n",
    "        that were predicted correctly.\n",
    "    '''\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        #tf.summary.scalar(scope.name+'/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
