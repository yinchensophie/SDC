{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model layer by layer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, define layer module that we will use.\n",
    "\n",
    "Here we define max-pooling layer and convolutional layer. \n",
    "\n",
    "Kernal size, strides and padding in max-pooling layer are fixed while in conv. layer are parametric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_2x2(inputs):\n",
    "    return tf.nn.max_pool(inputs, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], \n",
    "                          padding = 'VALID')\n",
    "\n",
    "def add_conv_layer(inputs, in_size, out_size, k_size, n_layer, activation_function=None):\n",
    "    layer_name = 'conv%s' % n_layer\n",
    "    with tf.name_scope(layer_name):\n",
    "        Weights = tf.get_variable('weights_%s'%n_layer,\n",
    "                                  shape = [k_size, k_size, in_size, out_size],\n",
    "                                  dtype = tf.float32, \n",
    "                                  initializer=tf.truncated_normal_initializer(\n",
    "                                      stddev=0.1,dtype=tf.float32))\n",
    "        #tf.histogram_summary(layer_name + '/weights', Weights)\n",
    "            \n",
    "        biases = tf.get_variable('biases_%s'%n_layer,\n",
    "                                 shape=[out_size],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        #tf.histogram_summary(layer_name + '/biases', biases)\n",
    "                 \n",
    "        pre_act = tf.nn.bias_add(tf.nn.conv2d(inputs, Weights, strides=[1, 1, 1, 1], \n",
    "                                              padding='SAME'), biases)\n",
    "        \n",
    "        if activation_function is None:\n",
    "            conv_layer = pre_act\n",
    "        else:\n",
    "            conv_layer = activation_function(pre_act)\n",
    "           \n",
    "        #tf.histogram_summary(layer_name + '/outputs', outputs)        \n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, build the net!\n",
    "\n",
    "I didn't define fc layer as a module.\n",
    "Because the usage of fc layer is not as much as conv. layer and max-pooling. \n",
    "\n",
    "Below shows a net with 8 conv. layers followed by max-pooling every 2 conv. layers and 3 fc layers.\n",
    "\n",
    "Note that the idea of this structure is coming from VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, batch_size, n_classes, on_train):\n",
    "    '''\n",
    "    Args:\n",
    "        images: image batch, 4D tensor, tf.float32, [batch_size, width, height, channels]\n",
    "    Returns:\n",
    "        output tensor with the computed logits, float, [batch_size, n_classes]\n",
    "    '''\n",
    "    #inputs, in_size, out_size, k_size, n_layer, activation_function=None, bn=None\n",
    "    conv1 = add_conv_layer(images, 3, 32, 3, n_layer=1, activation_function=tf.nn.relu, bn=None) \n",
    "    conv2 = add_conv_layer(conv1, 32, 32, 3, n_layer=2, activation_function=tf.nn.relu, bn=None) \n",
    "    pool2 = max_pool_2x2(conv2)\n",
    "\n",
    "    conv3 = add_conv_layer(pool2, 32, 64, 3, n_layer=3, activation_function=tf.nn.relu, bn=None) \n",
    "    conv4 = add_conv_layer(conv3, 64, 64, 3, n_layer=4, activation_function=tf.nn.relu, bn=None) \n",
    "    pool4 = max_pool_2x2(conv4)\n",
    "\n",
    "    conv5 = add_conv_layer(pool4, 64, 128, 3, n_layer=5, activation_function=tf.nn.relu, bn=None) \n",
    "    conv6 = add_conv_layer(conv5, 128,128, 3, n_layer=6, activation_function=tf.nn.relu, bn=None) \n",
    "    pool6 = max_pool_2x2(conv6)\n",
    "\n",
    "    conv7 = add_conv_layer(pool6, 128, 256, 3, n_layer=7,  activation_function=tf.nn.relu, bn=None) \n",
    "    conv8 = add_conv_layer(conv7, 256, 256, 3, n_layer=8, activation_function=tf.nn.relu, bn=None) \n",
    "    pool8 = max_pool_2x2(conv8)\n",
    "\n",
    "    #fc9  ,   14*14*256 -> 4096\n",
    "    with tf.variable_scope('fc9') as scope:\n",
    "        reshape = tf.reshape(pool8, shape=[batch_size, -1])\n",
    "        dim = reshape.get_shape()[1].value\n",
    "\n",
    "        #Sophie added for testing\n",
    "        print(reshape.get_shape)\n",
    "        print(dim)\n",
    "\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[dim,4096],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[4096],\n",
    "                                 dtype=tf.float32, \n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        act_fc9 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "        fc9 = tf.nn.dropout(act_fc9, keep_prob=0.5)    \n",
    "        \n",
    "    #fc10 \n",
    "    with tf.variable_scope('fc10') as scope:\n",
    "        weights = tf.get_variable('weights',\n",
    "                                  shape=[4096,1024],\n",
    "                                  dtype=tf.float32, \n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases',\n",
    "                                 shape=[1024],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        act_fc10 = tf.nn.relu(tf.matmul(fc9, weights) + biases, name='fc10')\n",
    "        fc10 = tf.nn.dropout(act_fc10, keep_prob=0.5)\n",
    "        \n",
    "        \n",
    "    # fc11 + softmax  1024 -> 3\n",
    "    with tf.variable_scope('fc11_softmax') as scope:\n",
    "        weights = tf.get_variable('softmax_linear',\n",
    "                                  shape=[1024, n_classes],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.truncated_normal_initializer(stddev=0.005,dtype=tf.float32))\n",
    "        biases = tf.get_variable('biases', \n",
    "                                 shape=[n_classes],\n",
    "                                 dtype=tf.float32, \n",
    "                                 initializer=tf.constant_initializer(0.1))\n",
    "        softmax_linear = tf.add(tf.matmul(fc10, weights), biases, name='softmax_linear')\n",
    "        #prediction = tf.nn.softmax(softmax_linear) \n",
    "    \n",
    "    #return prediction\n",
    "    return softmax_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
