{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training function\n",
    "\n",
    "This file contains training and testing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import input_data\n",
    "import model\n",
    "import model_layer\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 3\n",
    "IMG_W = 224  # resize the image. if input image is too large, training will be very slow.\n",
    "IMG_H = 224\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 2000\n",
    "MAX_STEP = 7000\n",
    "learning_rate = 0.001 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training flow\n",
    "\n",
    "Every 50 steps, it will show current loss and accuracy in terminal.\n",
    "Every 2000 steps, it will save current weight and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    \n",
    "    train_dir = '../pattern/2500/00627train'\n",
    "    logs_train_dir = 'logs'\n",
    "    \n",
    "    train, train_label = input_data.get_files(train_dir)\n",
    "    \n",
    "    train_batch, train_label_batch = input_data.get_batch(train,\n",
    "                                                          train_label,\n",
    "                                                          IMG_W,\n",
    "                                                          IMG_H,\n",
    "                                                          BATCH_SIZE, \n",
    "                                                          CAPACITY)      \n",
    "    train_logits = model_layer.inference(train_batch, BATCH_SIZE, N_CLASSES, 1)\n",
    "    train_loss = model.losses(train_logits, train_label_batch)        \n",
    "    train_op = model.trainning(train_loss, learning_rate)\n",
    "    train__acc = model.evaluation(train_logits, train_label_batch)\n",
    "       \n",
    "    #summary_op = tf.summary.merge_all()\n",
    "    sess = tf.Session()\n",
    "    #train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    try:\n",
    "        for step in np.arange(MAX_STEP):\n",
    "            if coord.should_stop():\n",
    "                    break\n",
    "            _, tra_loss, tra_acc = sess.run([train_op, train_loss, train__acc])\n",
    "               \n",
    "            if step % 50 == 0:\n",
    "                print('Step %d, train loss = %.2f, train accuracy = %.2f%%' %(step, tra_loss, tra_acc*100.0))\n",
    "                #summary_str = sess.run(summary_op)\n",
    "                #train_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            if step % 2000 == 0 or (step + 1) == MAX_STEP:\n",
    "                checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "                \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        \n",
    "    coord.join(threads)\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define testing flow\n",
    "\n",
    "Test all images in test_dir with saved model.\n",
    "\n",
    "1. While testing, it will copy error images to relative directory. For example, if the GT is 'nail' and the model's prediction is 'smear', this error image will be copied to 'result/error/nail_error/smear_nail/'.\n",
    "2. Note that this isn't a good method that repeat loading model for each test image. A better way is load once and test all.(The only reason I wrote in this way is I copied and modified it from another code. And when I modified, I didn't have enough time to improve it. So it's still waitting for someone to improve it XD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "LABELS = { 'nail':0, 'scratch':1 ,'smear':2 }\n",
    "GROUNDTRUTH = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_all_img_in_dirs():\n",
    "\n",
    "    test_dir = '../pattern/2500/00627test'\n",
    "    test, test_label = input_data.get_files(test_dir)\n",
    "\n",
    "    global GROUNDTRUTH    \n",
    "    count=0\n",
    "    nail_error=0\n",
    "    nail_cor=0\n",
    "    scratch_error=0\n",
    "    scratch_cor=0\n",
    "    smear_error=0\n",
    "    smear_cor=0\n",
    "\n",
    "    n = len(test)\n",
    "    for index in range(n):       \n",
    "        #ind = np.random.randint(0, n)\n",
    "        img_dir = test[index]\n",
    "        print(\"#----------------------------\")\n",
    "        print('image: ' + img_dir)\n",
    "        for key,value in LABELS.items():\n",
    "            if key in img_dir:\n",
    "                GROUNDTRUTH = value\n",
    "        image = Image.open(img_dir)\n",
    "        #plt.imshow(image)\n",
    "        image_array = np.array(image)\n",
    "        \n",
    " \n",
    "        with tf.Graph().as_default():\n",
    "            BATCH_SIZE = 1\n",
    "            N_CLASSES = 3\n",
    "    \n",
    "            image = tf.cast(image_array, tf.float32)\n",
    "            image = tf.image.resize_image_with_crop_or_pad(image, 224, 224)\n",
    "            image = tf.image.per_image_standardization(image)\n",
    "            image = tf.reshape(image, [1, 224, 224, 3])\n",
    "            logit = model_layer.inference(image, 1, N_CLASSES, 0)\n",
    "        \n",
    "            logit = tf.nn.softmax(logit)\n",
    "\n",
    "            logs_test_dir = 'logs' \n",
    "            #logs_test_dir = 'logs_stru6_0.9521_newdata' \n",
    "            #logs_test_dir = 'logs_stru6_noBN_0.911311' \n",
    "                       \n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "            with tf.Session() as sess:\n",
    "            \n",
    "                print(\"Reading checkpoints...\")\n",
    "                ckpt = tf.train.get_checkpoint_state(logs_test_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    print('Loading success, global_step is %s' % global_step)\n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "            \n",
    "                prediction = sess.run(logit)\n",
    "                max_index = np.argmax(prediction)\n",
    "                print('%.6f %.6f %.6f' %(prediction[:, 0],prediction[:, 1],prediction[:, 2]))\n",
    "                if max_index==0:\n",
    "                    print('This is a nail defect with possibility %.6f' %prediction[:, 0])\n",
    "                elif max_index == 1:\n",
    "                    print('This is a scratch defect with possibility %.6f' %prediction[:, 1])\n",
    "                else:\n",
    "                    print('This is a smear defect with possibility %.6f' %prediction[:, 2])\n",
    "\n",
    "                #global count, nail_error, scratch_error, smear_error, nail_cor, scratch_cor, smear_cor\n",
    "                if max_index == GROUNDTRUTH:\n",
    "                    count += 1\n",
    "                    if max_index == 0:\n",
    "                        nail_cor += 1\n",
    "                    elif max_index == 1:\n",
    "                        scratch_cor += 1\n",
    "                    elif max_index == 2:\n",
    "                        smear_cor += 1\n",
    "                elif GROUNDTRUTH == 0:\n",
    "                    nail_error += 1\n",
    "                    #print('nail_error_image: ' + img_dir)\n",
    "                    if max_index == 1: # scratch_nail\n",
    "                        cp_wrong_sample('scratch', 'nail', img_dir)\n",
    "                    elif max_index == 2: # smear_nail\n",
    "                        cp_wrong_sample('smear', 'nail', img_dir)\n",
    "                elif GROUNDTRUTH == 1:\n",
    "                    scratch_error += 1\n",
    "                    #print('scratch_error_image: ' + img_dir)\n",
    "                    if max_index == 0: # nail_scratch\n",
    "                        cp_wrong_sample('nail', 'scratch', img_dir)\n",
    "                    elif max_index == 2: # smear_scratch\n",
    "                        cp_wrong_sample('smear', 'scratch', img_dir)\n",
    "                elif GROUNDTRUTH == 2:\n",
    "                    smear_error += 1\n",
    "                    #print('smear_error_image: ' + img_dir)\n",
    "                    if max_index == 0: # nail_smear\n",
    "                        cp_wrong_sample('nail', 'smear', img_dir)\n",
    "                    elif max_index == 1: # scratch_smear\n",
    "                        cp_wrong_sample('scratch', 'smear', img_dir)\n",
    "\n",
    "    print(\"accuracy: %f\" %(float(count/n)))\n",
    "    print(\"nail    ---error: %d, ---correct: %d\"    % (nail_error, nail_cor))\n",
    "    print(\"scratch ---error: %d, ---correct: %d\"    % (scratch_error, scratch_cor))\n",
    "    print(\"smear   ---error: %d, ---correct: %d\"    % (smear_error, smear_cor))\n",
    "\n",
    "\n",
    "def cp_wrong_sample(err, cor, img_file):\n",
    "    commend = 'cp ' + img_file + ' result/error/' + cor + '_error/' + err + '_' + cor +'/'\n",
    "    subprocess.call(commend, shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to do training and inference !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_training()\n",
    "\n",
    "test_all_img_in_dirs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
